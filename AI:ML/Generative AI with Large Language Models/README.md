# [Generative AI with LLMs](https://www.deeplearning.ai/courses/generative-ai-with-llms/)

In Generative AI with Large Language Models (LLMs), you‚Äôll learn the fundamentals of how generative AI works, and how to deploy it in real-world applications.

In this course, I've had the privilege of delving deep into the world of Generative AI and Large Language Models, learning the fundamentals and practical applications that are shaping the future of AI. Here's what I've gained:

üîç Comprehensive Understanding: I now have a solid grasp of generative AI and its real-world applications. From data gathering and model selection to performance evaluation and deployment, I've covered the entire generative AI lifecycle.

ü§ñ Transformer Magic: I've explored the inner workings of the transformer architecture that powers Large Language Models. Understanding how these models are trained and fine-tuned for specific use cases has been a game-changer.

üìà Optimization Skills: I've learned to optimize a model's objective function using empirical scaling laws, considering factors like dataset size, compute resources, and inference requirements.

üõ†Ô∏è Hands-on Experience: With practical training, fine-tuning, and deployment methods, I'm now equipped with the tools and knowledge to maximize model performance within project-specific constraints.

üåü Industry Insights: Hearing from industry researchers and practitioners, I've gained valuable insights into the challenges and opportunities that generative AI presents for businesses

## Week 1
Generative AI use cases, project lifecycle, and model pre-training

### Learning Objectives
- Discuss model pre-training and the value of continued pre-training vs fine-tuning
- Define the terms Generative AI, large language models, prompt, and describe the transformer architecture that powers LLMs
- Describe the steps in a typical LLM-based, generative AI model lifecycle and discuss the constraining factors that drive decisions at each step of model lifecycle
- Discuss computational challenges during model pre-training and determine how to efficiently reduce memory footprint
- Define the term scaling law and describe the laws that have been discovered for LLMs related to training dataset size, compute budget, inference requirements, and other factors

[Lab 1 - Generative AI Use Case: Summarize Dialogue](https://github.com/Ryota-Kawamura/Generative-AI-with-LLMs/blob/main/Week-1/Lab_1_summarize_dialogue.ipynb)



## Week 2
Fine-tuning and evaluating large language models

### Learning Objectives
- Describe how fine-tuning with instructions using prompt datasets can improve performance on one or more tasks
- Define catastrophic forgetting and explain techniques that can be used to overcome it
- Define the term Parameter-efficient Fine Tuning (PEFT)
- Explain how PEFT decreases computational cost and overcomes catastrophic forgetting
- Explain how fine-tuning with instructions using prompt datasets can increase LLM performance on one or more 

[Lab 2 - Fine-tune a generative AI model for dialogue summarization](https://github.com/Ryota-Kawamura/Generative-AI-with-LLMs/blob/main/Week-2/Lab_2_fine_tune_generative_ai_model.ipynb)


## Week 3
Reinforcement learning and LLM-powered applications

### Learning Objectives
- Describe how RLHF uses human feedback to improve the performance and alignment of large language models
- Explain how data gathered from human labelers is used to train a reward model for RLHF
- Define chain-of-thought prompting and describe how it can be used to improve LLMs reasoning and planning abilities
- Discuss the challenges that LLMs face with knowledge cut-offs, and explain how information retrieval and augmentation techniques can overcome these challenges

[Lab 3 - Fine-tune FLAN-T5 with reinforcement learning to generate more-positive summaries](https://github.com/Ryota-Kawamura/Generative-AI-with-LLMs/blob/main/Week-3/Lab_3_fine_tune_model_to_detoxify_summaries.ipynb)
